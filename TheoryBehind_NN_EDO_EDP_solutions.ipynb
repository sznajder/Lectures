{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "TheoryBehind_NN_EDO_EDP_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/TheoryBehind_NN_EDO_EDP_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCooOPDFbovi"
      },
      "source": [
        "# Theory behind `PyDEns`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0kGYZdgbovl"
      },
      "source": [
        "Welcome! This tutorial explains math behind **`PyDEns`** ~-- framework for solving partial differential equations (PDEs) using neural networks, introducing the very details of equation solving. Our library is heavily inspired by the paper [DGM: A deep learning algorithm for solving partial differential equations](http://arxiv.org/abs/1708.07469), but is capable of doing way more: it is applicable to parametric equations, to equations with uncertainty, and can even be used to inforce prior information on solution via adjusting imperfect coefficients. This notebook introduces basic concepts, and is structed as follows:\n",
        "\n",
        "* [Why do we use Neural Networks?](#why)\n",
        "* [PDE problem setup](#problem)\n",
        "* [Galerkin Method](#galerkin)\n",
        "* [Neural Networks as approximators](#theorems)\n",
        "* [Deep Galerkin method](#dg)\n",
        "* [Tracting the intractable](#sampling)\n",
        "* [Ansatz](#ansatz)\n",
        "* [Advanced topics](#advanced)\n",
        "\n",
        "We do recommend to check out this notebook; yet, if you want to go straight to PDE solving, you can go right to [the next tutorial](1.%20Basic%20PDE%20solving.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7FldmMbovm"
      },
      "source": [
        "<a id='why'></a>\n",
        "## Why do we use Neural Networks?\n",
        "\n",
        "Before diving deep into the realm of deep learning, we need to understand exactly the reasons, why we can't be satisfied with more traditional methods. After all, *finite element*, *finite differences*, and many other approaches are under active development for the last hundred years, and should meet all the criteria of a good PDE solver, right? Not exactly. There are quite a few problems, related to both approaches itself and their software implementations:\n",
        "\n",
        "* most of popular numerical techniques operate on a fixed mesh. That greatly simplifies some of calculations and gives rigid structure, yet detrimental when solving problems that require very fine granularity: number of computations growth exponentially on cell size\n",
        "\n",
        "* working with uncertainty and parameters is both ineffective and ugly\n",
        "\n",
        "* in the recent years, accelerators like GPU and TPU have improved tremendously, yet most of PDE solving software packages does not utilize this potential to its full extent\n",
        "\n",
        "Usage of neural networks for PDE solving allows us to deal with most of these problems by simply borrowing modern tensor-processing frameworks like `TensorFlow` or `PyTorch`: they provide seamless integration of GPUs/TPUs into our workflow, greatly increasing the speed of inference.\n",
        "\n",
        "Being mesh-free is useful not only with regards to finding solution on a fine grid: it allows us to incorporate prior information directly into the learning process. We can easily 'tell' algorithm on which areas it should perform best, and also point out places that we don't care too much about (again, saving precious computational resources).\n",
        "\n",
        "At the first glance it might seem that NN usage in such classical problem of mathematics goes against centuries of progress in that domain. We will shortly see that, in fact, neural network-based approximators fit very harmoniously into the ensemble of existing approaches, enjoying various tricks both from emerging field of machine learning and well-estabilished theory of differential equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA-lKZxVbovm"
      },
      "source": [
        "<a id='problem'></a>\n",
        "## PDE problem setup\n",
        "\n",
        "We are set to find function $u(x, t)$ with respect to:\n",
        "\n",
        "$$ F\\left(u; t, x; \\nabla u; \\nabla^2 u; ...\\right) = 0, $$\n",
        "\n",
        "where $x$ is a $n$-dimensional vector, $t$ is time variable, which can be absent, and $F$ is some general functional. We use $\\nabla u$, $\\nabla^2 u$ to denote set of partial derivatives of the function of the first/second order respectively. For now, we would constraint $F$ to belong to a class of continuous functions, yet, as we would see, that is not strictly necessary.\n",
        "\n",
        "In order to make the solution unique, we must impose ***boundary*** and/or ***initial*** conditions\n",
        "\n",
        "$$ \\left. u(x, t) \\right\\rvert_{x \\in \\partial\\Omega} = g(x, t), $$\n",
        "\n",
        "$$  u(x, t_0) = u_0(x), $$\n",
        "\n",
        "$$  \\left. \\frac{\\partial u(x, t)}{\\partial t} \\right\\rvert_{t = t_0} = u_1(x), $$\n",
        "\n",
        "with $\\Omega$ being part of $\\mathcal{R}^{n+1}$.\n",
        "\n",
        "**Note:** we also use $F[u]$ for brevity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExcyA_ebovm"
      },
      "source": [
        "<a id='galerkin'></a>\n",
        "## Galerkin method\n",
        "\n",
        "**`PyDEns`** is based upon **DeepGalerkin** method. Unsurprisingly, the latter is based on an time-tested **Galerkin** method and it is indeed time to revisit this, one of the traditional methods of solving functional (not only differential) equations. The idea behind this approach is to approximate the unknown function with a finite linear combination of pre-defined basis functions. In order to achieve the best in-class approximation, we select coefficients of linear combination that minimize some error function. To be more rigorous, we swap unknown function $u(x, t)$ with\n",
        "\n",
        "$$ \\widehat{u}(x, t) = \\sum_{i=0}^N \\alpha_i\\phi_i(x, t).$$\n",
        "\n",
        "We define error at one space point as \n",
        "\n",
        "$$ J(x, t; \\alpha) = \\left\\| F[\\widehat{u}]\\right\\| + \\left\\|\\widehat{u}(x, t) - g(x, t) \\right\\|_{x \\in \\partial\\Omega} +  \\left\\|\\widehat{u}(x, t) - u_0(x) \\right\\|_{t = t_0}, $$\n",
        "\n",
        "and in order to obtain optimal values for $\\alpha_i$ we must find $argmin$ of the integral over entire domain\n",
        "\n",
        "$$ \\int_{\\Omega} J(x, t; \\alpha) dx dt.$$\n",
        "\n",
        "Note that in most cases various families of orthogonal polynomials are used to form the basis. That is due to polynomials being [dense](https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem) in the space of continuos functions, which allows them to approximate them arbitrarily well (that is, with any desired precision).\n",
        "\n",
        "There are a lot of different ways to find optimal $\\alpha$ values: some of them are analytical, some of them are numerical. We will look at them closely later in regards to neural network approximation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v5ELsEIbovn"
      },
      "source": [
        "<a id='theorems'></a>\n",
        "## Neural Networks as approximators\n",
        "\n",
        "Now we need to know some underlying theory behind the representational power of neural networks. A well-known [result](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states, that any real-valued continuos function can be approximated as closely as desired on a compact by a feed-forward neural network with a single hidden layer, finite number of neurons and nonconstant continuos bounded activation function. In other words, feed-forward neural networks are dense in the space of continuos functions (just like polynomials!).\n",
        "\n",
        "It is worth noting, though, that the number of neurons needed to achieve error rate of $\\epsilon$ is exponential in the dimensionality of approximated function. On the other hand, it [has been shown](https://arxiv.org/abs/1908.09375), that functions that can be represented as composition of functions with lesser dimensionality, can be approximated to a desired tolerance with a much lower size.\n",
        "\n",
        "All that allows us to say that, from approximation point of view, neural networks are not *that* different from polynomials: instead of using ***degree*** as the sizing parameter of a polynomial, we use ***number of parameters (neurons)*** in the network for quite similar purposes, and enjoy its ability to closely follow any continuos real-valued function. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32l1WYeAbovn"
      },
      "source": [
        "<a id='dg'></a>\n",
        "## Deep Galerkin method\n",
        "\n",
        "As we now know, we can use neural network with set of parameters $\\theta$ as approximator. We can use multiple such networks as basis for ***Galerkin method***, but, actually, we can just use one and increase the size of it to control its representational power. Thus, our approach looks very simple:\n",
        "\n",
        "$$ \\widehat{u}(x, t) = Net(x, t; \\theta).$$\n",
        "\n",
        "Error functionals, both point-wise and domain-wide one, are given by\n",
        "\n",
        "\\begin{gather}\n",
        "    J(\\theta) = \\left\\|F[Net]\\right\\| + \\left\\|Net(x, t; \\theta) - g(x, t) \\right\\|_{x \\in \\partial\\Omega} + \\\\ +  \\left\\|Net(x, t; \\theta) - u_0(x) \\right\\|_{t = t_0},\n",
        "\\end{gather}\n",
        "\n",
        "$$\\int_{\\Omega} J(x, t; \\theta) dx dt. $$ \n",
        "\n",
        "We should minimize value of the error over  NN parameters $\\theta$ to achieve optimal neural networks weights. \n",
        "\n",
        "Before talking about actual minimization, we should look at even more apparent issue: in order to compute error (even for one point in space), we must evaluate $F[Net]$. Operator $F$, as defined before, contains derivatives of up to the second order of the inputs, which can cumbersome to calculate even for the simplest functions. We are potentially talking about neural networks with hundreds of layers, so how do we do that?\n",
        "\n",
        "Fortunately, we can use the same concepts that power up modern deep learning: [computational graphs and backpropogation](https://colah.github.io/posts/2015-08-Backprop/). Usually they are used to compute gradients of the network output with respect to its weights in order to adjust them to the task; we use this capability both to make the approximation better, as well as calculate values of differential form. If you are not familiar with these concepts, we highly recommend watching [this video](https://www.youtube.com/watch?v=-yhm3WdGFok)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq59TTS_bovn"
      },
      "source": [
        "<a id='sampling'></a>\n",
        "## Tracting the intractable\n",
        "\n",
        "The error function, given by an integral, is rarely tractable. It means that we can't just evaluate its value, so we swap the integral over continuos domain with summation over finite ***batch*** of points:\n",
        "\n",
        "$$ \\int_{\\Omega} J(x, t; \\theta)  dx dt \\longrightarrow \\sum_{x^i, t^i \\in B \\in \\Omega} J(x^i, t^i; \\theta). $$\n",
        "\n",
        "Note that we fully control the entire process of generating points from $\\Omega$: we can sample more points in promising regions, completely ignoring uninteresting ones. On the other hand, shall we need it, we can simulate any fixed grid: for example, it seems reasonable to sample points near boundaries/zero-time at the very beginning of the learning process, so we can mimic some conventional numerical methods. Even during the actual inference we can *fine-tune* the network on the fly right before using it to obtain approximation of the solution.\n",
        "\n",
        "**Note:** with great power comes great responsibility. Such massive freedom requires elaborate framework, and that is exactly what **`PyDEns`** is for!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlOgZxlcbovo"
      },
      "source": [
        "<a id='ansatz'></a>\n",
        "## Ansatz\n",
        "\n",
        "As was mentioned before, we can use not only modern techniques for training complex neural networks, but also some tricks from more classical domains of mathematics. One of them is ***ansatz***, which allows to explicitly bind initial and/or boundary conditions.\n",
        "\n",
        "Taking a look once again at \n",
        "\\begin{gather}\n",
        "    J(\\theta) = \\left\\|F[Net]\\right\\| + \\left\\|Net(x, t; \\theta) - g(x, t) \\right\\|_{x \\in \\partial\\Omega} + \\\\ +  \\left\\|Net(x, t; \\theta) - u_0(x) \\right\\|_{t = t_0},\n",
        "\\end{gather}\n",
        "we can see that in consists of multiple equal terms, so optimization process treats them, well, equally. Sometimes that can be detrimental: conditions must be satisfied exactly, with zero tolerance. Ansatz allows us to achieve such binding by applying a simple yet effective transformation to the approximator:\n",
        "\n",
        "$$ Net \\longrightarrow A(Net, g, u_0, u_1). $$\n",
        "\n",
        "Exact form of function $A$ depends on number of conditions to inforce. For example, in the simplest possible case, where $t_0=0$ and we only have condition on function in the initial time ($u_0$), we can use\n",
        "\n",
        "$$ A = u_0(x) + t \\cdot Net(x, t; \\theta)$$\n",
        "\n",
        "to explicitly enforce condition on approximator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdDuSciybovo"
      },
      "source": [
        "<a id='history'></a>\n",
        "## History\n",
        "\n",
        "In 1998, [Lagaris et al.](https://pdfs.semanticscholar.org/5ebb/e0b1a3d7a2431bbb25d6dfeec7ed6954d633.pdf) proposed to approximate PDE solutions with shallow neural networks. Unfortunately, backprop was not popular back then, so authors had to manually compute derivatives of network w.r.t. inputs. Due to complexities of the method and lack of evidence of practical benefits, this domain of research took a long pause.\n",
        "\n",
        "After development of convinient machine learning libraries like [TensorFlow](https://ai.google/research/pubs/pub45381), the interest has emerged once again. Most of the modern approaches can be traced back to [DeepGalerking by Sirignano and Spiliopoulos, 2018](https://arxiv.org/abs/1708.07469), which heavily inspired our framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aiZhlvAbovo"
      },
      "source": [
        "<a id='advanced'></a>\n",
        "## Advanced topics\n",
        "Proposed method readily extends to the case of multiple equations with multiple unknowns. Yet, a more exciting avenue is solving PDEs with uncertainties, parameters or to even propose novel settings of PDE solving. You can learn more about all these things [here](2.%20Advanced%20PyDEns.ipynb), but be sure to start with the [basics](1.%20Basic%20PDE%20solving.ipynb)!"
      ]
    }
  ]
}