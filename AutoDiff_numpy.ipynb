{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "AutoDiff_numpy.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/AutoDiff_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4c72c3090ed50e44364f41419b37ef723d022f18",
        "id": "Sav3lxcRRS0z"
      },
      "source": [
        "# Gentle Introduction to automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0eee393763cf2de4049d6647effff93bffe9afcc",
        "id": "Q6J8iTVORS03"
      },
      "source": [
        "Automatic differntiation is about computing derivatives of functions encoded as computer programs. In this notebook, we will build a skeleton of a toy autodiff framework in Python, using dual numbers and Python's magic methods.\n",
        "\n",
        "## Tl;dr summary ##\n",
        "* Automatic differentiation is a key component in every deep learning framework.\n",
        "* The basics of automatic differentiation are not hard.\n",
        "* Autograd package is awesome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d5d7309d75e6538f1dcc08974259eb66b429e8c6",
        "id": "le4WvgqwRS04"
      },
      "source": [
        "### Prior art\n",
        "There are plenty of resources about automatic differentiation. This notebook comes from my attempt to digest [this great blog post](https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/) by Alexey Radul. There've been plenty others, like this [article](http://www.ams.org/publicoutreach/feature-column/fc-2017-12) from AMS. In terms of code, [this github gist](https://gist.github.com/kmizu/1428717/b7ccee41e1d8ec62fbd2bd64df50bc8cb097d51c) is very much in the spirit as my code, albeit in Scala. Alexey Radul also teamed up with some autograd luminaries to write [a comprehensive survey](https://arxiv.org/abs/1502.05767) on automatic differentiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "81aa89aa212ad64d53896dc311847cbf70d5ab60",
        "id": "-NPzYfrvRS04"
      },
      "source": [
        "## Table of Contents \n",
        "[Section 1. Derivative in a picture](#picture)\n",
        "\n",
        "[Section 2. Building toy autodiff](#toy)\n",
        "\n",
        "  [2a. Python magic methods](#magic)\n",
        " \n",
        "[Section 3. How to build a real autodiff](#real)\n",
        "\n",
        "[Section 4. Questions and Exercises.](#qa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "30ef8e3d0388702e91affcb25c97b584eae9924f",
        "id": "PyBj6mbzRS04"
      },
      "source": [
        "# <a name=\"picture\"></a>Section 1. Derivative in a picture #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5745d3d2011975d2fbd41afe72315a5b05adf31d",
        "id": "iblVpy8ZRS04"
      },
      "source": [
        "We will pay some lip service to the notion of derivative - instead of defining it, we will use a picture and an animation to illustrate it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9e558cc01846ef42e7236fd5bd0ab0c8c97abfe6",
        "id": "K5xUaIJYRS05"
      },
      "source": [
        "Let's define a very simple function $$\\text{func}(x)=3x^3-5x^2.$$ \n",
        "The derivative of func can be readily computed to be $$\\text{func_der}(x)=9x^2-10x.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9a3c8224d07b188b6913cf928e117897cf4af2f9",
        "id": "mpKIo7z6RS05"
      },
      "source": [
        "#In Python code:\n",
        "def func(x):\n",
        "    return 3 * x ** 3 - 5 * x ** 2\n",
        "\n",
        "def func_der(x):\n",
        "    return 9 * x ** 2 - 10 * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5cd47fc10721b9a4680cf7d46de632331337a915",
        "id": "jItNxuMmRS05"
      },
      "source": [
        "What does derivative really mean? Let's imagine the graph of the function $\\text{func}$. Let's take a point $x=1.5$. We compute the derivative $$\\text{der_func}(1.5)= 9 * (1.5)^2 - 15 = 5.25.$$\n",
        "This informs us that the slope of the tangent line to the graph of $\\text{func}$ is 5.25. Indeed, we can plot this to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "471ecf25d2850766efb83d785672242a45456ef4",
        "id": "mT9nwii1RS05"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "29565ed44218f1a269746dd369ac3559a0ac1c7f",
        "id": "TymA4du9RS06"
      },
      "source": [
        "x = np.linspace(0,2,200)\n",
        "y = func(x)\n",
        "xprime = np.linspace(1.1,1.9)\n",
        "yprime = (xprime - 1.5) * func_der(1.5)  + func(1.5)\n",
        "plt.text(0.5, 2, \"$y=3x^3-5x^2$\", fontsize=20)\n",
        "plt.text(0.5,1, \"$y_{line}=5.25x-9$\", fontsize=16)\n",
        "plt.axvline(1.5, color='k', linestyle='--',linewidth=1) \n",
        "plt.plot(x,y, xprime, yprime, 'r--', [1.5], [func(1.5)], 'ro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7747bd43c8d7c7a19c6a9131707f77e0c5e372a4",
        "id": "e30QMPNORS06"
      },
      "source": [
        "### Cauchy's definition of derivative animated ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "12256d1bc599ea65f29baf05df9a9f7dc98e5e72",
        "id": "Dq0xnA28RS06"
      },
      "source": [
        "What is the precise definition of the derivative? It involves a limit, so instead of writing this down, let's use an animation to illustrate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "066b5c1a371d5b48c0eb98d3a85ce420fec613bf",
        "id": "WJaREnmtRS06"
      },
      "source": [
        "#Initial plot to setup the animation\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlim(( 0, 2))\n",
        "ax.set_ylim((-4, 4))\n",
        "_,_,_, point, line = ax.plot(x,y, xprime, yprime, 'r--', [1.5], [func(1.5)], 'ro', [],[], 'ko', [], [], 'k-')\n",
        "text = ax.text(0.5, 1, \"\")\n",
        "ax.text(0.5, 0.65, \"derivative 5.25\", color=\"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d28b76cd702b4a7441f24f7a88199fed33d35bef",
        "id": "km4OxzWzRS06"
      },
      "source": [
        "Watch in the animation below how when the other end point approaches $x=1.5$, the slope of the arc becomes closer to the value given by the derivatvive $\\text{der_func}(1.5)=5.25$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "61fe298d57eb865403adb0a142312000507630cd",
        "id": "1JfPOzrXRS06"
      },
      "source": [
        "def init():\n",
        "    line.set_data([], [])\n",
        "    point.set_data([], [])\n",
        "    text.set_text(\"\")\n",
        "    return (point, line, text)\n",
        "def animate(i):\n",
        "    if (i < 45):\n",
        "        pt = 1.495 - 0.495 * (60 - i) / 60\n",
        "    elif (i < 75):\n",
        "        pt = 1.495 - 0.495 * (16.25 - (i-45)/2 ) / 75\n",
        "    elif (i<80):\n",
        "        pt = 1.495\n",
        "    elif (i < 125):\n",
        "        pt = 1.495 + 0.495 * (140 - i) / 60\n",
        "    elif (i < 155):\n",
        "        pt = 1.495 + 0.495 * (16.25 - (i-125)/2 ) / 75\n",
        "    else:\n",
        "        pt = 1.505\n",
        "    x = np.linspace(0.8, 1.99)\n",
        "    text.set_text(\"slope of the arc {0:.4f}\".format((func(1.5) - func(pt))/(1.5 - pt)))\n",
        "    y = (x - 1.5) * (func(1.5) - func(pt))/(1.5 - pt) + func(1.5)\n",
        "    line.set_data(x, y)\n",
        "    point.set_data([pt], [func(pt)])\n",
        "    return (point, line, text)\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
        "                               frames=160, repeat=True, blit=True)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fe24f9ad3eedc27b358559acc11989ce47a8ab3a",
        "id": "w6mSSJ-cRS07"
      },
      "source": [
        "## Computation of derivatives - motivation ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "24dac666f42dca8485210401e1e030aca8d40bb4",
        "id": "hluE1vEDRS07"
      },
      "source": [
        "So why do we seek to compute derivatives? There are quite a few uses for that\n",
        "\n",
        "* Sensitivity analysis\n",
        "\n",
        "Derivative is an indicator of a rate of change.\n",
        "\n",
        "* Optimization\n",
        "\n",
        "Can I find the minimum, maximum value that my code returns? Fastest optimization algorithms are based on computing derivatives.\n",
        "* Finding inverses\n",
        "\n",
        "What is the inputs for my code that return 5?\n",
        "\n",
        "For example, the function <i>square root</i> is the inverse of the function <i>square</i>. Indeed, [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method) to compute the inverse of function $f$ consists of iterating \n",
        "$$\n",
        "x_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_n)}\n",
        "$$\n",
        "until convergence and it uses derivatives.\n",
        "\n",
        "* Machine Learning and Deep Learning\n",
        "\n",
        "Machine learning training task (mainly supervised) is simply finding parameters that minimize a certain chosen function called loss. In deep learning, the number of parameters can get to billions, hence the need for smart ways to compute derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "597e5f0866cc269700d0be76b0f6837887869aee",
        "id": "SUkb08cqRS07"
      },
      "source": [
        "# <a name=\"build\"></a> Section 2. Building toy autodiff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7ef3dbb962052dfdee95ce48a66be648f51c259b",
        "id": "STNmNHZhRS07"
      },
      "source": [
        "One of the way to approach automatic differentiation is to use dual numbers. What are they?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b826230c5d30974a73f091535dd8c2cf110420d8",
        "id": "IeKgqBKXRS07"
      },
      "source": [
        "## Dual numbers\n",
        "Suppose now $f$ is a nice enough function and $\\varepsilon$ is a small number. Then one can write\n",
        "$$ f(x + \\varepsilon) = f(x) + \\varepsilon f'(x) + \\varepsilon^2 ... + \\varepsilon^3 ... + ....$$\n",
        "This is essentially Taylor's theorem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "4a4fefb65ea62cd4b78b72377b736b68b4ea34cb",
        "id": "kUsqRL0ARS07"
      },
      "source": [
        "### Stepping outside of math for a moment\n",
        "Suppose $\\varepsilon$ is so small that $$\\varepsilon^2=0$$ then $$f(x + \\varepsilon) = f(x) + \\varepsilon f'(x).$$\n",
        "So armed object like $x + \\varepsilon$, we can plug it in to computation of $f$ and then read of the derivative from the coefficient of the $\\varepsilon$ term. Simple, isn't it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0c0f8ab5599e29c8bc7468c79e4581d4065f2a13",
        "id": "3ZbReUY7RS08"
      },
      "source": [
        "### Defining dual numbers\n",
        "Let's try to define this intuition. Dual number consists of a pair of real numbers, $(val, eps)$. In math notation one would write $val + eps \\cdot\\varepsilon$. Think of it as a Python object *Dual(val, eps)*. Define this in code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bb76ddcd4fe908dedfc93e15d93e6623ea04d5c6",
        "id": "jLkts-1CRS08"
      },
      "source": [
        "class DualBasic(object):\n",
        "    def __init__(self, val, eps):\n",
        "        self.val = val\n",
        "        self.eps = eps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9d151d1316d59121e75e63ed1ff6316f9586faf9",
        "id": "vITz_isnRS08"
      },
      "source": [
        "# <a name=\"magic\"></a> 2a. Python magic methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9e6fc057d129483e3355b1bb21401ed2140d4eb9",
        "id": "xE_WSvw1RS08"
      },
      "source": [
        "At the next step, we would like manipulations of dual numbers to look as much as possible as manipulation of regular floats. In order to achieve that, we will use Python's `magic methods`. Magic methods  allow overloading/extending basic operators and functions to new classes by using special names for attributes and methods that serve as hooks for many python standard functions. See this [great tutorial](https://rszalski.github.io/magicmethods/) and the [official documentation](https://docs.python.org/3/reference/datamodel.html#special-method-names)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "19b5cd9fe25f7c5e9a0851a5bb610afb783587fa",
        "id": "njlsd37WRS08"
      },
      "source": [
        "For convenience sake, we will add an automatic coversion of ```float``` or ```int``` number ```x``` to ```Dual(x, 0)```. And also add the absolute value function and string representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "08d6024474d72276d0b44fb6e60908e2e6efdc55",
        "id": "_mGE6MibRS08"
      },
      "source": [
        "class DualBasicEnhanced(object):\n",
        "    def __init__(self, *args):\n",
        "        if len(args) == 2:\n",
        "            value, eps = args\n",
        "        elif len(args) == 1:\n",
        "            if isinstance(args[0], (float, int)):\n",
        "                value, eps = args[0], 0\n",
        "            else:\n",
        "                value, eps = args[0].value, args[0].eps\n",
        "        self.value = value\n",
        "        self.eps = eps\n",
        "        \n",
        "    def __abs__(self):\n",
        "        return abs(self.value)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"Dual({}, {})\".format(self.value, self.eps)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e0de8384051d3b7bd81af5fdcb5a2e88e77d38e3",
        "id": "ziWYQ6uKRS09"
      },
      "source": [
        "How should arithmetic work? The arithmetic operations should come from $\\varepsilon^2=0$. So we define:\n",
        "\n",
        "Addition\n",
        "$$(x + a \\varepsilon)+(y + b\\varepsilon)=(x+y) + (a+b)\\varepsilon$$\n",
        "Multiplication\n",
        "$$(x + a \\varepsilon)*(y + b\\varepsilon)=xy + (xb+ya)\\varepsilon$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "459cf0538b291170d78e9bfe8f6005274e597658",
        "id": "W7E_VH-5RS09"
      },
      "source": [
        "#In code:\n",
        "class DualArith(object):\n",
        "    def __add__(self, other):\n",
        "        other = Dual(other)\n",
        "        return Dual(self.value + other.value, self.eps + other.eps)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        other = Dual(other)\n",
        "        return Dual(self.value - other.value, self.eps - other.eps)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        other = Dual(other)\n",
        "        return Dual(self.value * other.value, self.eps * other.value + self.value * other.eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0abee37c797a9169e13dcc6a9d8793983937e70a",
        "id": "b35m2Yk9RS09"
      },
      "source": [
        "Very important is the division of two dual numbers. There are two ways to arrive to what should the inverse of a dual number be. I'll give hints in the exercises. For now let's define:\n",
        "\n",
        "Division\n",
        "$$ \\frac{1}{x+a\\varepsilon} = \\frac{1}{x} - \\varepsilon\\frac{a}{x^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ad07ce2ddbb0c8d4896e16a9b3d84acceb5d38a7",
        "id": "aS4JJjwoRS09"
      },
      "source": [
        "class DualDiv(object):\n",
        "        def __truediv__(self, other):\n",
        "            other = Dual(other)\n",
        "            if abs(other.value) == 0:\n",
        "                raise ZeroDivisionError\n",
        "            else:\n",
        "                return Dual(self.value / other.value, \n",
        "                            self.eps / other.value - self.value / (other.value)**2 * other.eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3b821ab2e9adf313f20aabc7afcb897d13fb577d",
        "id": "QWBXJt-wRS09"
      },
      "source": [
        "We have enough to perform some basic computations with our class. Let's bring the all together in one class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8ec2b126a8deeb7c78aca12041499a2f4ab2ddb0",
        "id": "GitZDMfFRS09"
      },
      "source": [
        "class Dual(DualBasicEnhanced, DualArith, DualDiv):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "26e709622933b48d7d15c229b21c52decdc34887",
        "id": "goNEUpypRS0-"
      },
      "source": [
        "__Now to the main point:__\n",
        "## Statement: ##\n",
        "Suppose a Python function *piece_of_code* approximates a mathematical function *f*. Then\n",
        "\n",
        "\n",
        "<center>*piece_of_code(Dual(x,1))*</center>\n",
        "approximates\n",
        "<center>\n",
        "*Dual(f(x), f'(x))*</center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "10b5490c449dc96d4cf726f081b5b8803f494b7f",
        "id": "We9x7EgRRS0-"
      },
      "source": [
        "We'll spend the rest of this section to demonstrate this statement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8b94f1e030f47b4c87fa22b94168873aa2733c4f",
        "id": "IhTfi6_WRS0-"
      },
      "source": [
        "The most important class to verify this is for polynomials. Why? Because a lot of other functions can be approximated by polynomials:\n",
        "\n",
        "\n",
        "For example $$e^x \\approx \\sum_{k=0}^n \\frac{1}{k!} x^k.$$ Take $n=4$: \n",
        "$$e^x = 1 + x + \\frac{1}{2} x^2 + \\frac{1}{6} x^3 + O(x^4).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fea41b8e77057c2e486bd3457992d4581dd28cbc",
        "id": "RI7NeyKURS0-"
      },
      "source": [
        "### Demonstrate for polynomials ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d04f72aa89bb5824e596a10e2dd930e3d8c5d705",
        "id": "gl4lwnCdRS0-"
      },
      "source": [
        "So a derivative of a monomial $x^k$ where $k$ is an integer number is $$(x^k)'=kx^{k-1}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f9e656a74c5285a4609e0cd047b1296f60216e79",
        "id": "P7qDtQR5RS0-"
      },
      "source": [
        "So derivative of $x^2$ is $2x$ which at $x=3$ is $2*3=6$. With our dual numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5a07f030f1f3bc8ad7c23796bb8458b446a6ee68",
        "id": "eMjRDo5hRS0-"
      },
      "source": [
        "def square(x):\n",
        "    return x * x\n",
        "square(Dual(3,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "03e4372037c88aa677dd6745557b8a73b627ec20",
        "id": "5yNcBJIkRS0-"
      },
      "source": [
        "Great. (Recall that to read off the derivative, you need to look at the second number)\n",
        "\n",
        "Let's try the derivavive of $x^3$, which at 2 is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1317eadf7f33e837730f6f2862111a133dd94c88",
        "id": "X_cohhyIRS0-"
      },
      "source": [
        "def cube(x):\n",
        "    return x * x * x\n",
        "cube(Dual(2,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e8a7a62f69f65c77f379d770c94598cb863b3b16",
        "id": "zRZp36wdRS0-"
      },
      "source": [
        "Fantastic!\n",
        "\n",
        "Also important, for  a constant $c$ the derivative of $cx$ is just $c$. So for $c=2$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7ce323bc3e7fd232be4b394c065d4bdd8234f582",
        "id": "JlZ0KUwsRS0_"
      },
      "source": [
        "def by2(x):\n",
        "    return x * 2\n",
        "by2(Dual(5,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "40881fbcc3233fde2dd28fabf0e4c60417eb9836",
        "id": "yJkIFdoXRS0_"
      },
      "source": [
        "What about other functions that are not computed with polynomial approximation? Example - square root. Square root is computed by Newton formula. To compute the square root of $y$, we seek the fixed point of the function $$f(x) = \\frac12 x + \\frac12 \\frac yx$$. I.e. the square root of $y$ is $x_0$ such that\n",
        "$$x_0 = \\frac12 x_0 + \\frac12 \\frac y{x_0}$$\n",
        "How do you compute an approximation to a fixed point? Iterate the function until the results are close to each other. Indeed, if we build a sequence $x_0, x_1, ..., $ by $x_{i+1} = f(x_{i})$ and we have for some $n$ that $x_{n+1}\\approx x_n$ then substituting $f(x_n)\\approx x_n$ which means that $x_n \\approx \\sqrt{y}$ which is what we are trying to achieve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bcf52a4f6a1d89bf6d43f9972f5ab49ddf3b8fb9",
        "id": "zhy8BYHORS0_"
      },
      "source": [
        "EPS = 10E-12 # arbitrary accuracy\n",
        "\n",
        "def next_iter(xn, ysq):\n",
        "    return (xn + ysq / xn) * 1/2\n",
        "\n",
        "def custom_sqrt(ysq):\n",
        "    xnext, xprev = ysq, 0\n",
        "    while abs(xnext * xnext - xprev * xprev) > EPS * abs(ysq):\n",
        "        xnext, xprev = next_iter(xnext, ysq), xnext\n",
        "    return xnext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "26b262a378327df04c308bba4b4134c28e1d608e",
        "id": "87MnAX8ZRS0_"
      },
      "source": [
        "Let's check that our function works as intended"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "15f417e6d81444ecb014bd7c2847f7fe45bc7021",
        "id": "XDa1IjVNRS0_"
      },
      "source": [
        "custom_sqrt(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "66b257c194382f59405b0e1a29d5644537cf3ab0",
        "id": "Eold21TYRS0_"
      },
      "source": [
        "What should we expect for the derivative? Recall the computation of the derivative of square root:\n",
        "$$ (\\sqrt{x})' =(x^\\frac{1}{2})'=\\frac{1}{2} x^{\\frac{1}{2} - 1} = \\frac{1}{2 \\sqrt{x}}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "62cb975715c43eac1fad82f8bfe518e03f017b1a",
        "id": "c45fKZXxRS0_"
      },
      "source": [
        "custom_sqrt(Dual(4,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2a00ac9a0fe6ee57a11faa170b357d0e886ded3a",
        "id": "Nwi4mcAVRS0_"
      },
      "source": [
        "Just the result we were expecting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a38d246417345338d8896d3d4dbfdb41a7b5449f",
        "id": "QqvchuIHRS0_"
      },
      "source": [
        "## <a name=\"real\"></a> Section 3. How to create an automatic differentiation system?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6f9025e7b22b345f0c5a302180ef488f9da4eb1d",
        "id": "ouml_kytRS0_"
      },
      "source": [
        "Unfortunately, this is as far as our framework can go. If we tried this on a real life Python function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "caea721fc5006e1dd6849f9feef3e7e73f4145b9",
        "id": "EevRUowHRS0_"
      },
      "source": [
        "from math import sqrt\n",
        "sqrt(Dual(4,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0ef838bdcd16c693a65f1a43d346a3ecb6f74d18",
        "id": "wDvJigIQRS1A"
      },
      "source": [
        "This is because many Python function are written in C and don't operate well on custom objects such as our Dual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8d609b3b16d3925ee7e2932760d8461174c8549e",
        "id": "F7QU5-vDRS1A"
      },
      "source": [
        "### So how does one implement automatic differentiation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5b14f5f611119885ee3ff85da07681706a3b7371",
        "id": "-llVYxybRS1A"
      },
      "source": [
        "- Operator overloading/Templates/Generify - this works well for statically typed languages. Esp. for Scala with its' powerful `implicits` mechanism.\n",
        "- Source to source translation - programmatically inspect the code and replace the computations of floats with computations of derivatives. Google's [Tangent](https://github.com/google/tangent) is one such attempt. It uses `Autograd` and `Tensorflow`'s eager mode of computation to achieve its goals.\n",
        "- Write dedicated automatic differentiation framework:\n",
        "    - We can add hooks for derivatives that we know. For instance $(e^x)'=e^x$, it is inefficient to recompute it. In fact, one can say that our toy autodiff is such a framework with hooks only for arithmetic operations. That's why we had to go to all that trouble to implement a custom square root. \n",
        "    - Can build a function from basic building blocks if our framework is rich enough. Example from `Pytorch` with forward (computation of the function) and backward (computation of the derivative) methods. Also `Tensorflow`, `Theano` and every deep learning framework out there. And of course `Autograd`.\n",
        "- `Autograd` goes further than the deep learning frameworks - it swaps numpy with it's own extended version allowing to seamlessly differentiate numpy code without hassle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "737e822b9d61c2bc8ec1c1b2fd9fec5285c73310",
        "id": "wATzlY88RS1A"
      },
      "source": [
        "You can use Autograd to compute derivatives of ```custom_sqrt```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "40f054a253c255c0dc9eab5983159f9e58cc459f",
        "id": "i6SUKDbxRS1A"
      },
      "source": [
        "import autograd\n",
        "grad_custom_sqrt = autograd.grad(custom_sqrt)\n",
        "grad_custom_sqrt(4.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "e9427014b63503dc230c1663fcbf80c960e6c470",
        "id": "7TS7R2vzRS1A"
      },
      "source": [
        "Of course, there are no miracles here. Trying to differentiate Python's ```math.sqrt``` fill fail:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "94a6460ce2fa4e479c2d099c7128918b2e2a2384",
        "id": "NlogosbtRS1A"
      },
      "source": [
        "import math\n",
        "grad_math_sqrt = autograd.grad(math.sqrt)\n",
        "try:\n",
        "    grad_math_sqrt(4.)\n",
        "except:\n",
        "    import traceback\n",
        "    traceback.print_exc(limit=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "980b2e31b1c71064189a1f9d0c3218f94d4cfba0",
        "id": "pMCCJn73RS1A"
      },
      "source": [
        "Instead, we should use the wrapper around ```numpy``` to achieve the desired result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "655abd4bfad4ff942a2c3c1c628259ccf0f89801",
        "id": "tb6wLtoARS1A"
      },
      "source": [
        "import autograd.numpy as np\n",
        "autograd.grad(np.sqrt)(4.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7868cc6fee58bf361e56a9122c715fd7d85471e5",
        "id": "_OmXvOiWRS1A"
      },
      "source": [
        "For Pytorch, the situation is similar, we could use our ```custom_sqrt``` to build graph in Pytorch (with version 0.4 API improvements):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f59911cae4de468a73c35738435b6b25734d4fb2",
        "id": "3U_HeVPxRS1A"
      },
      "source": [
        "Create a pytorch tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9de2af7559a8cf167b6901ef02862a180345a053",
        "id": "eH1g87BhRS1A"
      },
      "source": [
        "import torch\n",
        "x = torch.tensor(4., requires_grad=True)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8ea36e742036f930382f34d3f90df635d45fdda3",
        "id": "WQLt4NatRS1B"
      },
      "source": [
        "Build the graph with ```custom_sqrt``` and differentiate it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "1d0c632e4a9bd76f6f15d1dcae17e5aaafaa0bf3",
        "id": "LwttrAedRS1B"
      },
      "source": [
        "graph = custom_sqrt(x)\n",
        "graph.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9239e36f5e571648e1567c4f418c2c8539508960",
        "id": "6F1-QSN4RS1B"
      },
      "source": [
        "And the derivative is..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c680f942e5f844b40788fbee96ee09512c588d21",
        "id": "I9DYUFKsRS1B"
      },
      "source": [
        "x.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "95561f44c44d1f16af19dd8d255d3b70baed60c5",
        "id": "kI2ibocLRS1B"
      },
      "source": [
        "## <a name=\"qa\"></a> Section 4. Questions and Exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c057b1bd588cc2b5b0b0b83fec1aca7f4606be13",
        "id": "WHAvhaNgRS1B"
      },
      "source": [
        "1. How to establish that $ Dual(x,1)=Dual(\\frac{1}{x}, -\\frac{1}{x^2})$ or in math notation $\\frac{1}{x+\\varepsilon}=\\frac{1}{x}-\\frac{1}{x^2}\\varepsilon$? There are two ways, in fact. One is to use the derivative of the function $f(x)=\\frac{1}{x}$. The more satisfying way is purely algebraic - by solving an equation. We need to find two real numbers $x_1, x_2$ such that: $$ (x+\\varepsilon)(x_1 + x_2\\varepsilon)= 1 + 0 \\cdot \\varepsilon. $$\n",
        "    \n",
        "1. The approximation $e^x \\approx 1 + x + \\frac{x^2}{2} + \\frac{x^3}{3!}+\\frac{x^4}{4!}$ breaks down pretty quickly when $x$ is large enough. To compute the exponential for large $x$, we can employ the following procedure. Use $x$'s binary expansion to write it as a sum of powers of two and a small remainder:\n",
        "$$\n",
        "x = \\sum_{b_i> -5} 2^{b_i} + y,\n",
        "$$\n",
        "where $b_i$'s are non-zero bits of the binary expansion of $x$. We chose to terminate our expansion at -5 in an arbitrary fashion. We then use $e^{a+b}=e^{a}e^b$ to write \n",
        "$$\n",
        "e^x = e^y\\big( e^{2^{b_m}}..e^{2^{b_0}}\\big).\n",
        "$$\n",
        "We can then apply the polynomial approximation for $e^{y}$ (in fact $e^{y}\\approx 1+y$ works great). Verify that when implemented like this with say $compute\\_exp$ then $compute\\_exp(Dual(x,1))$ produces the correct derivative.\n",
        "\n",
        "1. a. Is our toy-autodiff approach a \"forward mode\" or \"backward mode\" differentiation? See [Wikipedia article](https://en.wikipedia.org/wiki/Automatic_differentiation) for defintions.\n",
        "\n",
        "    b. Why would ```custom_sqrt``` be impossible in Tensorflow (or Tensorflow's default, original mode) and definitely not in Theano.\n",
        "    \n",
        "    c. If you familiar with either of those rewrite a compromise version of ```custom_sqrt```.\n",
        "\n",
        "1. Note how we were always using ```Dual(a,b) * c``` for multiplication by a scalar, which looks a little bit awkward. What is Python magic method do we need to implement to make ```c * Dual(a,b)``` work?\n",
        "    \n",
        "1. Prove that for a polynomial $P(x)=\\sum a_n x^n$ and your favorite method to compute polynomials the identity $P(Dual(x,1)) = Dual(P(x), P'(x))$ holds. \n",
        "    This will help to explain why the following happens:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "a932842b896aaa02cfe0c4e68af0fb1f1e3b224d",
        "id": "a2oO6B2cRS1B"
      },
      "source": [
        "from sympy.abc import x\n",
        "def func(var):\n",
        "    return var * var * var * 3 - var * var * 5\n",
        "func(Dual(x,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "390a67e10aec0f1f7766e78a06d5dcafb141d8a8",
        "id": "e6npat9_RS1B"
      },
      "source": [
        "6.Another interesting point to think about is many-variable functions. After all, neural networks have many millions of parameters. For concreteness, take the two-variable function below and compute it's gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2edc83bf587498a2b891bb914bf9150cb78abb70",
        "id": "a3lw_hY9RS1B"
      },
      "source": [
        "def two_var_func(x,y):\n",
        "    return y/custom_sqrt(x*x + y*y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b63a1ccb8ef338c74b8ac6829a13a1cd48ffef56",
        "id": "SWpQrlqNRS1B"
      },
      "source": [
        "There are two ways to go about this: either compute the directional derivatives or use two-dimensional perturbations correctly. If you don't want to rewrite the ```Dual``` class, you can use ```numpy``` for vector perturbations. Of course at this point our approach will deviate sharply from the deep learning package's way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b53b5b6b4884e58100231e6d67fb848674e5ae5a",
        "id": "nkgDvIoiRS1C"
      },
      "source": [
        "## The end ##\n",
        "### Hope you enjoyed, comments and questions are welcome. ###"
      ]
    }
  ]
}