{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Set2Graph_GNN_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/Set2Graph_GNN_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Gxp-RgzWPR"
      },
      "source": [
        "## Set2Graph Demo\n",
        "\n",
        "### A novel set-to-graph model which takes into account information from all tracks in a jet to determine if pairs of tracks originated from a common vertex. It can be used in JetID for b and c jet tagging ( E.Gross, K.Cranmer , J.Shlomi & all ,  https://arxiv.org/pdf/2008.02831.pdf )\n",
        "\n",
        "\n",
        "Cloned from J.R.Vlimant\n",
        "https://github.com/vlimant/NNArchTeraScale2021/blob/master/graphs/set2graph.ipynb\n",
        "\n",
        "\n",
        "credits to **Jonathan Shlomi**. Model from the paper https://arxiv.org/abs/2008.02831 with more code available at https://github.com/jshlomi/SetToGraphPaper"
      ],
      "id": "d5Gxp-RgzWPR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KIQaFI3zWPU"
      },
      "source": [
        "Download the jet dataset from https://zenodo.org/record/4044628 ; do this only once"
      ],
      "id": "1KIQaFI3zWPU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ATxueUzWPU",
        "outputId": "a2d3baba-8ab3-42dd-983c-ee50ea298415"
      },
      "source": [
        "!curl -O https://zenodo.org/record/4044628/files/valid_data.root"
      ],
      "id": "q_ATxueUzWPU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  114M  100  114M    0     0  3641k      0  0:00:32  0:00:32 --:--:-- 3660k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1zDP-hMzWPU"
      },
      "source": [
        "## the dataloader code\n",
        "\n",
        "each jet has a different number of tracks, but the model needs all entries in a batch to have the same shape. so we use a sampler to pick jets with the same number of tracks"
      ],
      "id": "R1zDP-hMzWPU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h07W1t-QzWPV",
        "outputId": "a823bbc5-9951-4855-f720-73b202a319a4"
      },
      "source": [
        "import os\n",
        "import uproot\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "import setGPU"
      ],
      "id": "h07W1t-QzWPV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setGPU: Setting GPU to: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc6J8e10zWPV"
      },
      "source": [
        "class JetGraphDataset(Dataset):\n",
        "    def __init__(self, fname):\n",
        "        self.node_features_list = ['trk_d0', 'trk_z0', 'trk_phi', 'trk_ctgtheta', 'trk_pt', 'trk_charge']\n",
        "    \n",
        "        self.jet_features_list = ['jet_pt', 'jet_eta', 'jet_phi', 'jet_M']\n",
        "\n",
        "\n",
        "        with uproot.open(fname) as f:\n",
        "            tree = f['tree']\n",
        "            self.n_jets = int(tree.numentries)\n",
        "            self.n_nodes = np.array([len(x) for x in tree['trk_vtx_index'].array()])\n",
        "\n",
        "            self.jet_arrays = tree.arrays(self.jet_features_list + self.node_features_list + ['trk_vtx_index'])#,library='np')\n",
        "            self.sets, self.partitions, self.partitions_as_graphs = [], [], []\n",
        "\n",
        "        \n",
        "        for set_, partition, partition_as_graph in self.get_all_items():\n",
        "            #if torch.cuda.is_available():\n",
        "            #    set_ = torch.tensor(set_, dtype=torch.float, device='cuda')\n",
        "            #    partition = torch.tensor(partition, dtype=torch.long, device='cuda')\n",
        "            #    partition_as_graph = torch.tensor(partition_as_graph, dtype=torch.float, device='cuda')\n",
        "            self.sets.append(set_)\n",
        "            self.partitions.append(partition)\n",
        "            self.partitions_as_graphs.append(partition_as_graph)\n",
        "\n",
        "        #if not torch.cuda.is_available():\n",
        "        #    self.sets = np.array(self.sets,dtype=object)\n",
        "        #    self.partitions = np.array(self.partitions,dtype=object)\n",
        "        #    self.partitions_as_graphs = np.array(self.partitions_as_graphs,dtype=object)\n",
        "\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.n_jets\n",
        "\n",
        "    def get_all_items(self):\n",
        "        node_feats = np.array([self.jet_arrays[x.encode()] for x in self.node_features_list])\n",
        "        jet_feats = np.array([self.jet_arrays[x.encode()] for x in self.jet_features_list])\n",
        "        n_labels = np.array(self.jet_arrays['trk_vtx_index'.encode()])\n",
        "\n",
        "        for i in range(self.n_jets):\n",
        "            n_nodes = self.n_nodes[i]\n",
        "            node_feats_i = np.stack(node_feats[:, i], axis=0)  # shape (6, n_nodes)\n",
        "            jet_feats_i = jet_feats[:, i]  # shape (4, )\n",
        "            jet_feats_i = jet_feats_i[:, np.newaxis]  # shape (4, 1)\n",
        "\n",
        "            node_feats_i = transform_features(FeatureTransform.node_feature_transform_list, node_feats_i)\n",
        "            jet_feats_i = transform_features(FeatureTransform.jet_features_transform_list, jet_feats_i)\n",
        "\n",
        "            jet_feats_i = np.repeat(jet_feats_i, n_nodes, axis=1)  # change shape to (4, n_nodes)\n",
        "            set_i = np.concatenate([node_feats_i, jet_feats_i]).T  # shape (n_nodes, 10)\n",
        "\n",
        "            partition_i = n_labels[i]\n",
        "\n",
        "            sort_order = np.argsort(node_feats_i[4])\n",
        "            set_i = set_i[sort_order]\n",
        "\n",
        "            tile = np.tile(partition_i, (self.n_nodes[i], 1))\n",
        "            partition_as_graph_i = np.where((tile - tile.T), 0, 1)\n",
        "\n",
        "            yield set_i, partition_i, partition_as_graph_i\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        return self.sets[idx], self.partitions[idx], self.partitions_as_graphs[idx]\n",
        "\n",
        "\n",
        "class JetsBatchSampler(Sampler):\n",
        "    def __init__(self, n_nodes_array, batch_size):\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "        :param n_nodes_array: array of sizes of the jets\n",
        "        :param batch_size: batch size\n",
        "        \"\"\"\n",
        "        super().__init__(n_nodes_array.size)\n",
        "\n",
        "        self.dataset_size = n_nodes_array.size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.index_to_batch = {}\n",
        "        self.node_size_idx = {}\n",
        "        running_idx = -1\n",
        "\n",
        "        for n_nodes_i in set(n_nodes_array):\n",
        "\n",
        "            if n_nodes_i <= 1:\n",
        "                continue\n",
        "            self.node_size_idx[n_nodes_i] = np.where(n_nodes_array == n_nodes_i)[0]\n",
        "\n",
        "            n_of_size = len(self.node_size_idx[n_nodes_i])\n",
        "            n_batches = max(n_of_size / self.batch_size, 1)\n",
        "\n",
        "            self.node_size_idx[n_nodes_i] = np.array_split(np.random.permutation(self.node_size_idx[n_nodes_i]),\n",
        "                                                           n_batches)\n",
        "            for batch in self.node_size_idx[n_nodes_i]:\n",
        "                running_idx += 1\n",
        "                self.index_to_batch[running_idx] = batch\n",
        "\n",
        "        self.n_batches = running_idx + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch_order = np.random.permutation(np.arange(self.n_batches))\n",
        "        for i in batch_order:\n",
        "            yield self.index_to_batch[i]\n",
        "\n",
        "def transform_features(transform_list, arr):\n",
        "    new_arr = np.zeros_like(arr)\n",
        "    for col_i, (mean, std) in enumerate(transform_list):\n",
        "        new_arr[col_i, :] = (arr[col_i, :] - mean) / std\n",
        "    return new_arr\n",
        "\n",
        "class FeatureTransform(object):\n",
        "    # Based on mean and std values of TRAINING set only\n",
        "    node_feature_transform_list = [\n",
        "        (0.0006078152, 14.128961),\n",
        "        (0.0038490593, 10.688491),\n",
        "        (-0.0026713554, 1.8167108),\n",
        "        (0.0047640945, 1.889725),\n",
        "        (5.237357, 7.4841413),\n",
        "        (-0.00015662189, 1.0)]\n",
        "\n",
        "    jet_features_transform_list = [\n",
        "        (75.95093, 49.134453),\n",
        "        (0.0022607117, 1.2152709),\n",
        "        (-0.0023569583, 1.8164033),\n",
        "        (9.437994, 6.765137)]"
      ],
      "id": "Oc6J8e10zWPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-wXdffvzWPV"
      },
      "source": [
        "ds = JetGraphDataset('valid_data.root')"
      ],
      "id": "g-wXdffvzWPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnelDD8qzWPW",
        "outputId": "570f0baa-af17-4492-8470-c0ed56db66bf"
      },
      "source": [
        "ds[0]"
      ],
      "id": "mnelDD8qzWPW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.0018891 , -0.00236357,  0.42327377, -0.21854699,  0.5066499 ,\n",
              "          1.0001566 , -0.27195626, -0.3864688 ,  0.41081807, -0.53265476],\n",
              "        [ 0.00198516, -0.00227709,  0.38614634, -0.24547258,  1.5638206 ,\n",
              "         -0.99984336, -0.27195626, -0.3864688 ,  0.41081807, -0.53265476]],\n",
              "       dtype=float32),\n",
              " array([0, 0], dtype=int32),\n",
              " array([[1, 1],\n",
              "        [1, 1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlPRfA1VzWPW"
      },
      "source": [
        "the dataset outputs a $n_{tracks} \\times d_{in} $ array, a $n_{tracks}$ target array of vertex indices, and a $n\\times n$ array of edge targets"
      ],
      "id": "AlPRfA1VzWPW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpLilcguzWPW"
      },
      "source": [
        "batch_size = 10\n",
        "batch_sampler = JetsBatchSampler(ds.n_nodes, batch_size)\n",
        "data_loader = DataLoader(ds, batch_sampler=batch_sampler)"
      ],
      "id": "OpLilcguzWPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhAlBWRgzWPW"
      },
      "source": [
        "## the model\n",
        "\n",
        "the model is comprised of three parts, the set function $\\phi$, the broadcasting part $\\beta$ and the output edge prediction $\\psi$\n",
        "\n",
        "$\\phi$ is the DeepSet module\n",
        "\n",
        "$\\beta$ is implemented in the forward of the SetToGraph module\n",
        "\n",
        "$\\psi$ is the PsiSuffix module.\n",
        "\n",
        "\n",
        "the SetToGraph module combined all three"
      ],
      "id": "XhAlBWRgzWPW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs3CgAO2zWPW"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class DeepSet(nn.Module):\n",
        "    def __init__(self, in_features, feats):\n",
        "        super(DeepSet, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        layers.append(DeepSetLayer(in_features, feats[0]))\n",
        "        for i in range(1, len(feats)):\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(DeepSetLayer(feats[i-1], feats[i]))\n",
        "\n",
        "        self.sequential = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sequential(x)\n",
        "\n",
        "\n",
        "class DeepSetLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "      \n",
        "        super(DeepSetLayer, self).__init__()\n",
        "\n",
        "        self.attention = Attention(in_features)\n",
        "        self.layer1 = nn.Conv1d(in_features, out_features, 1)\n",
        "        self.layer2 = nn.Conv1d(in_features, out_features, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (B,C,N)\n",
        "\n",
        "        x_T = x.transpose(2, 1)  # B,C,N -> B,N,C\n",
        "        x = self.layer1(x) + self.layer2(self.attention(x_T).transpose(1, 2))\n",
        "       \n",
        "        x = x / torch.norm(x, p='fro', dim=1, keepdim=True)  # BxCxN / Bx1xN\n",
        "\n",
        "        return x\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super().__init__()\n",
        "        small_in_features = max(math.floor(in_features/10), 1)\n",
        "        self.d_k = small_in_features\n",
        "\n",
        "        self.query = nn.Sequential(\n",
        "            nn.Linear(in_features, small_in_features),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.key = nn.Linear(in_features, small_in_features)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp.shape should be (B,N,C)\n",
        "        q = self.query(inp)  # (B,N,C/10)\n",
        "        k = self.key(inp)  # B,N,C/10\n",
        "\n",
        "        x = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)  # B,N,N\n",
        "\n",
        "        x = x.transpose(1, 2)  # (B,N,N)\n",
        "        x = x.softmax(dim=2)  # over rows\n",
        "        x = torch.matmul(x, inp)  # (B, N, C)\n",
        "        return x\n",
        "    \n"
      ],
      "id": "bs3CgAO2zWPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdlVf2DqzWPX"
      },
      "source": [
        "class PsiSuffix(nn.Module):\n",
        "    def __init__(self, features):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(features) - 2):\n",
        "            layers.append(nn.Conv2d(features[i], features[i + 1],1))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Conv2d(features[-2], features[-1], 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "\n",
        "class SetToGraph(nn.Module):\n",
        "    def __init__(self, in_features, out_features, set_fn_feats, hidden_mlp):\n",
        "\n",
        "        super(SetToGraph, self).__init__()\n",
        "        \n",
        "\n",
        "        self.set_model = DeepSet(in_features=in_features, feats=set_fn_feats)\n",
        "      \n",
        "\n",
        "        # Suffix - from last number of features, to 1 feature per entrance\n",
        "        d2 = 3*set_fn_feats[-1]\n",
        "        hidden_mlp = [d2] + hidden_mlp + [out_features]\n",
        "        self.suffix = PsiSuffix(hidden_mlp)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(2, 1)  # from BxNxC to BxCxN\n",
        "        u = self.set_model(x)  # Bx(out_features)xN\n",
        "        n = u.shape[2]\n",
        "\n",
        "       \n",
        "        \n",
        "        m1 = u.unsqueeze(2).repeat(1, 1, n, 1)  # broadcast to rows\n",
        "        m2 = u.unsqueeze(3).repeat(1, 1, 1, n)  # broadcast to cols\n",
        "        m3 = torch.sum(u, dim=2, keepdim=True).unsqueeze(3).repeat(1, 1, n, n)  # sum over N, put on all\n",
        "\n",
        "        block = torch.cat((m1, m2, m3), dim=1)\n",
        "        edge_vals = self.suffix(block)  # shape (B,out_features,N,N)\n",
        "\n",
        "        return edge_vals.squeeze(1)\n",
        "\n",
        "\n"
      ],
      "id": "QdlVf2DqzWPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbn_MEvczWPX"
      },
      "source": [
        "model = SetToGraph(10,1,[256,256,256],[256])"
      ],
      "id": "pbn_MEvczWPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4TT93t7zWPX",
        "outputId": "2de3e81c-b75c-4301-b645-1cefb95faad7"
      },
      "source": [
        "ds[34][0].shape"
      ],
      "id": "m4TT93t7zWPX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ukgo4qYzWPX"
      },
      "source": [
        "### Putting one element from the dataset through the model:"
      ],
      "id": "9Ukgo4qYzWPX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJX5xIV1zWPX",
        "outputId": "2803539d-9aea-470c-9a05-1c2fbe670a3a"
      },
      "source": [
        "model( torch.tensor([ds[34][0]]) ).shape"
      ],
      "id": "VJX5xIV1zWPX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 14, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5YwJdAWzWPX"
      },
      "source": [
        "### basic training loop"
      ],
      "id": "A5YwJdAWzWPX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ4SXHV3zWPX",
        "outputId": "05215a65-c608-4831-d4a7-6d6bc23b1627"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def loss_func(y_hat, y):\n",
        "    \n",
        "    # No loss on diagonal, so set diagonal elements to 1\n",
        "    B, N, _ = y_hat.shape\n",
        "    y_hat[:, torch.arange(N), torch.arange(N)] = torch.finfo(y_hat.dtype).max  # to be \"1\" after sigmoid\n",
        "\n",
        "    # calc loss\n",
        "    loss = F.binary_cross_entropy_with_logits(y_hat, y)  # cross entropy\n",
        "\n",
        "    y_hat = torch.sigmoid(y_hat)\n",
        "    tp = (y_hat * y).sum(dim=(1, 2))\n",
        "    fn = ((1. - y_hat) * y).sum(dim=(1, 2))\n",
        "    fp = (y_hat * (1. - y)).sum(dim=(1, 2))\n",
        "    loss = loss - ((2 * tp) / (2 * tp + fp + fn + 1e-10)).sum()  # fscore\n",
        "\n",
        "    return loss\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1):\n",
        "    for graph, _, edge_target in data_loader:\n",
        "    \n",
        "        edge_prediction = model(graph)\n",
        "    \n",
        "        loss = loss_func(edge_prediction,edge_target.float())\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        break\n",
        "    print(\"Epoch:\",epoch)\n",
        "    \n",
        "    "
      ],
      "id": "iJ4SXHV3zWPX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC9_GjfBzWPX"
      },
      "source": [
        ""
      ],
      "id": "JC9_GjfBzWPX",
      "execution_count": null,
      "outputs": []
    }
  ]
}