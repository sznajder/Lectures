{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InteractionNetwork_BoostedHiggs_TFKeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhc7xyMa2jHzpfCiW5Zcef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Notebooks/blob/master/InteractionNetwork_BoostedHiggs_TFKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYkoiOOcghDx"
      },
      "source": [
        "# Extracted from PYTHON code. Needs to be organized for Jupyter NB\n",
        "# From: https://github.com/thongonary/LEIA-Net/blob/master/interaction.py\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFvaxBSQgEpw"
      },
      "source": [
        "\"\"\"\n",
        "Tensorflow implementation of the Interaction networks for the identification of boosted Higgs to bb decays https://arxiv.org/abs/1909.12285 \n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "from lbn import LBNLayer\n",
        "\n",
        "class LEIA(models.Model):\n",
        "    def __init__(self, n_constituents, n_targets, params, hidden, fr_activation=0, fo_activation=0, fc_activation=0, De=8, Do=8, sum_O=True, debug=False):\n",
        "        super(LEIA, self).__init__()\n",
        "\n",
        "        # initialize the LBN layer for preprocessing\n",
        "        self.lbn = LBNLayer(n_particles=n_constituents, n_restframes=n_constituents, boost_mode='pairs')\n",
        "\n",
        "        self.hidden = int(hidden)\n",
        "        self.P = params\n",
        "        self.N = self.lbn.lbn.n_out\n",
        "        self.Nr = self.N * (self.N - 1)\n",
        "        self.Dr = 0\n",
        "        self.De = De\n",
        "        self.Dx = 0\n",
        "        self.Do = Do\n",
        "        self.n_targets = n_targets\n",
        "        self.fr_activation = fr_activation\n",
        "        self.fo_activation = fo_activation\n",
        "        self.fc_activation = fc_activation \n",
        "        self.assign_matrices()\n",
        "        self.Ra = tf.ones([self.Dr, self.Nr])\n",
        "        self.fr1 = layers.Dense(self.hidden) #, input_shape=(2 * self.P + self.Dr,)\n",
        "        self.fr2 = layers.Dense(int(self.hidden/2)) # , input_shape=(self.hidden,)\n",
        "        self.fr3 = layers.Dense(self.De) # , input_shape=(int(self.hidden/2),)\n",
        "        \n",
        "        self.fo1 = layers.Dense(self.hidden) # , input_shape=(self.P + self.Dx + (2 * self.De),)\n",
        "        self.fo2 = layers.Dense(int(self.hidden/2)) # , input_shape=(self.hidden,)\n",
        "        self.fo3 = layers.Dense(self.Do) # , input_shape=(int(self.hidden/2),)\n",
        "        \n",
        "        self.fc1 = layers.Dense(hidden)\n",
        "        self.fc2 = layers.Dense(int(hidden/2))\n",
        "        self.fc3 = layers.Dense(self.n_targets)\n",
        "        self.sum_O = sum_O \n",
        "        self.debug = debug\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        self.built = True\n",
        "\n",
        "    def assign_matrices(self):\n",
        "        Rr = np.zeros([self.N, self.Nr], dtype=np.float32)\n",
        "        Rs = np.zeros([self.N, self.Nr], dtype=np.float32)\n",
        "        receiver_sender_list = [i for i in itertools.product(range(self.N), range(self.N)) if i[0]!=i[1]]\n",
        "        for i, (r, s) in enumerate(receiver_sender_list):\n",
        "            Rr[r, i] = 1\n",
        "            Rs[s, i] = 1\n",
        "        self.Rr = tf.convert_to_tensor(Rr)\n",
        "        self.Rs = tf.convert_to_tensor(Rs)\n",
        "        del Rs, Rr\n",
        "\n",
        "    def call(self, x):\n",
        "        '''\n",
        "        Expect input to have shape of (batches, N_particles, N_features)\n",
        "        '''\n",
        "        ###PF Candidate - PF Candidate###\n",
        "        if self.debug: \n",
        "            print(\"input_shape = {}\".format(x.shape))\n",
        "            print(f\"x before lbn : {x[0,0,:]}\")\n",
        "        x = self.lbn(x) # Already in E, px, py, pz # Bypass for now, just to check if the IN works\n",
        "        if self.debug: \n",
        "            print(f\"x after lbn : {x[0,0,:]}\\n\")\n",
        "            print(\"input_shape after lbn = {}\".format(x.shape))\n",
        "            print(\"n_outs after lbn = {}\".format(self.lbn.lbn.n_out))\n",
        "        x = tf.transpose(x, perm=[0, 2, 1]) # to fit in the IN\n",
        "        if self.debug: print(f\"x after transpose = {x.shape}\")\n",
        "        Orr = self.tmul(x, self.Rr)\n",
        "        if self.debug: print(f\"Orr = {Orr.shape}\")\n",
        "        Ors = self.tmul(x, self.Rs)\n",
        "        if self.debug: print(f\"Ors = {Ors.shape}\")\n",
        "        B = tf.concat([Orr, Ors], 1)\n",
        "        if self.debug: \n",
        "            print(f\"B = {B.shape}\")\n",
        "            print(f\"params = {self.P}\")\n",
        "        ### First MLP ###\n",
        "        B = tf.transpose(B, perm=[0, 2, 1])\n",
        "        if self.fr_activation == 2:\n",
        "            B = tf.nn.selu(self.fr1(tf.reshape(B, [-1, 2 * self.P + self.Dr])))\n",
        "            B = tf.nn.selu(self.fr2(B))\n",
        "            E = tf.nn.selu(tf.reshape(self.fr3(B), [-1, self.Nr, self.De]))\n",
        "        elif self.fr_activation == 1:\n",
        "            B = tf.nn.elu(self.fr1(tf.reshape(B, [-1, 2 * self.P + self.Dr])))\n",
        "            B = tf.nn.elu(self.fr2(B))\n",
        "            E = tf.nn.elu(tf.reshape(self.fr3(B), [-1, self.Nr, self.De]))\n",
        "        else:\n",
        "            B = tf.nn.relu(self.fr1(tf.reshape(B, [-1, 2 * self.P + self.Dr])))\n",
        "            if self.debug: print(f\"B after fr1 = {B.shape}\")\n",
        "            B = tf.nn.relu(self.fr2(B))\n",
        "            if self.debug: print(f\"B after fr2 = {B.shape}\")\n",
        "            E = tf.nn.relu(tf.reshape(self.fr3(B), [-1, self.Nr, self.De]))\n",
        "            if self.debug: print(f\"E after fr3 = {E.shape}\")\n",
        "        del B\n",
        "        if self.debug: print(\"E after 1st MLP = {}\".format(E.shape))\n",
        "        E = tf.transpose(E, perm=[0, 2, 1])\n",
        "        if self.debug:\n",
        "            print(\"E after transpose = {}\".format(E.shape))\n",
        "            print(\"Rr after transpose = {}\".format(self.Rr.shape))\n",
        "        Ebar = self.tmul(E, tf.transpose(self.Rr, perm=[1, 0]))\n",
        "        if self.debug: print(\"Ebar after tmul = {}\".format(Ebar.shape))\n",
        "        del E\n",
        "       \n",
        "        ####Final output matrix for particles###\n",
        "        C = tf.concat([x, Ebar], 1)\n",
        "        del Ebar\n",
        "        C = tf.transpose(C, perm=[0, 2, 1])\n",
        "        \n",
        "        ### Second MLP ###\n",
        "        if self.fo_activation == 2:\n",
        "            C = tf.nn.selu(self.fo1(tf.reshape(C, [-1, self.P + self.Dx + self.De])))\n",
        "            C = tf.nn.selu(self.fo2(C))\n",
        "            O = tf.nn.selu(tf.reshape(self.fo3(C), [-1, self.N, self.Do]))\n",
        "        elif self.fo_activation == 1:\n",
        "            C = tf.nn.elu(self.fo1(tf.reshape(C, [-1, self.P + self.Dx + self.De])))\n",
        "            C = tf.nn.elu(self.fo2(C))\n",
        "            O = tf.nn.elu(tf.reshape(self.fo3(C), [-1, self.N, self.Do]))\n",
        "        else:\n",
        "            C = tf.nn.relu(self.fo1(tf.reshape(C, [-1, self.P + self.Dx + self.De])))\n",
        "            C = tf.nn.relu(self.fo2(C))\n",
        "            O = tf.nn.relu(tf.reshape(self.fo3(C), [-1, self.N, self.Do]))\n",
        "        del C\n",
        "       \n",
        "        if self.sum_O:\n",
        "            O = tf.reduce_sum(O, 1)\n",
        "\n",
        "        ### Classification MLP ###\n",
        "        if self.fc_activation == 2:\n",
        "            if self.sum_O:\n",
        "                N = tf.nn.selu(self.fc1(tf.reshape(O, [-1, self.Do * 1])))\n",
        "            else:\n",
        "                N = tf.nn.selu(self.fc1(tf.reshape(O, [-1, self.Do * N])))\n",
        "            N = tf.nn.selu(self.fc2(N))   \n",
        "        if self.fc_activation == 1:\n",
        "            if self.sum_O:\n",
        "                N = tf.nn.elu(self.fc1(tf.reshape(O, [-1, self.Do * 1])))\n",
        "            else:\n",
        "                N = tf.nn.elu(self.fc1(tf.reshape(O, [-1, self.Do * N])))\n",
        "            N = tf.nn.elu(self.fc2(N)) \n",
        "        else:\n",
        "            if self.sum_O:\n",
        "                N = tf.nn.relu(self.fc1(tf.reshape(O, [-1, self.Do * 1])))\n",
        "            else:\n",
        "                N = tf.nn.relu(self.fc1(tf.reshape(O, [-1, self.Do * N])))\n",
        "            N = tf.nn.relu(self.fc2(N))\n",
        "        N = self.fc3(N)\n",
        "        return N\n",
        "\n",
        "    def tmul(self, x, y):  #Takes (I * J * K)(K * L) -> I * J * L \n",
        "        x_shape = tf.shape(x)\n",
        "        y_shape = tf.shape(y)\n",
        "        return tf.reshape(tf.matmul(tf.reshape(x, [-1, x_shape[2]]), y), [-1, x_shape[1], y_shape[1]]) \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhxlKRisgc4c"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpSkHc0gZJk"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imp\n",
        "import datetime\n",
        "try:\n",
        "    imp.find_module('setGPU')\n",
        "    import setGPU\n",
        "except ImportError:\n",
        "    pass    \n",
        "import glob\n",
        "import sys\n",
        "import tqdm\n",
        "import argparse\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from interaction import LEIA\n",
        "from data import H5Data\n",
        "import copy\n",
        "import h5py\n",
        "\n",
        "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
        "if os.path.isdir('/data/shared/hls-fml/'):\n",
        "    test_path = '/data/shared/hls-fml/NEWDATA/'\n",
        "    train_path = '/data/shared/hls-fml/NEWDATA/'\n",
        "elif os.path.isdir('/eos/project/d/dshep/hls-fml/'):\n",
        "    test_path = '/eos/project/d/dshep/hls-fml/'\n",
        "    train_path = '/eos/project/d/dshep/hls-fml/'\n",
        "\n",
        "N = 100 # number of particles\n",
        "n_targets = 5 # number of classes\n",
        "n_features = 4 # number of features per particles\n",
        "save_path = 'models/8/'\n",
        "best_path = save_path + '/best/'\n",
        "batch_size = 256\n",
        "n_epochs = 100\n",
        "\n",
        "files = glob.glob(train_path + \"/jetImage*_{}p*.h5\".format(N))\n",
        "num_files = len(files)\n",
        "files_val = files[:int(num_files*0.2)] # take first 20% for validation\n",
        "files_train = files[int(num_files*0.2):] # take rest for training\n",
        "files_trial = files[int(num_files*0.2):int(num_files*0.3)] \n",
        "data_train = H5Data(batch_size = batch_size,\n",
        "                    cache = None,\n",
        "                    preloading=0,\n",
        "                    features_name='jetConstituentList', \n",
        "                    labels_name='jets',\n",
        "                    spectators_name=None)\n",
        "data_val = H5Data(batch_size = batch_size,\n",
        "                  cache = None,\n",
        "                  preloading=0,\n",
        "                  features_name='jetConstituentList', \n",
        "                  labels_name='jets',\n",
        "                  spectators_name=None)\n",
        "\n",
        "            \n",
        "# Define loss function\n",
        "def loss(model, x, y):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    y_ = model(x)\n",
        "    return cce(y_true=y, y_pred=y_)\n",
        "\n",
        "def normalize(files, feature_name='jetConstituentList'):\n",
        "    # Return the mean and std for normalization\n",
        "    trial_file = h5py.File(files[0],\"r\")\n",
        "    sample = trial_file[feature_name]\n",
        "    print(f\"Getting mean and std from a sample of {len(sample)} events\")\n",
        "    sample = np.reshape(sample, [-1, sample.shape[-1]])\n",
        "    mean = np.mean(sample, axis=0).astype(np.float32)\n",
        "    std = np.std(sample, axis=0).astype(np.float32)\n",
        "    print(f\"Mean = {mean}\")\n",
        "    print(f\"Std = {std}\")\n",
        "    return mean, std\n",
        "\n",
        "def main(args):\n",
        "    \"\"\" Main entry point of the app \"\"\"\n",
        "    if args.trial: \n",
        "        data_train.set_file_names(files_trial)\n",
        "    else:    \n",
        "        data_train.set_file_names(files_train)\n",
        "    data_val.set_file_names(files_val)\n",
        "    \n",
        "    n_val=data_val.count_data()\n",
        "    n_train=data_train.count_data()\n",
        "\n",
        "    print(\"val data:\", n_val)\n",
        "    print(\"train data:\", n_train)\n",
        "\n",
        "    net_args = (N, n_targets, n_features, args.hidden)\n",
        "    net_kwargs = {\"fr_activation\": 0, \"fo_activation\": 0, \"fc_activation\": 0}\n",
        "    \n",
        "    gnn = LEIA(*net_args, **net_kwargs)\n",
        "    gnn.build(input_shape=(None, N, n_features))\n",
        "\n",
        "    # gnn.summary() # Doens't work. Seems like a common TF 2.0 issue: \n",
        "    # https://github.com/tensorflow/tensorflow/issues/22963 \n",
        "    # https://stackoverflow.com/questions/58182032/you-tried-to-call-count-params-on-but-the-layer-isnt-built-tensorflow-2-0\n",
        "\n",
        "    #### Start training ####\n",
        "    \n",
        "    # Keep results for plotting\n",
        "    train_loss_results = []\n",
        "    train_accuracy_results = []\n",
        "    val_loss_results = []\n",
        "    val_accuracy_results = []\n",
        "    \n",
        "    # Log directory for Tensorboard\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "    test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "    pathlib.Path(train_log_dir).mkdir(parents=True, exist_ok=True)  \n",
        "    pathlib.Path(test_log_dir).mkdir(parents=True, exist_ok=True)  \n",
        "\n",
        "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "    \n",
        "    # Load mean and std for normalization\n",
        "    mean, std = normalize(files_train)\n",
        "    \n",
        "    best_loss = 100\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        # Tool to keep track of the metrics\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "        epoch_accuracy = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\n",
        "        val_epoch_loss_avg = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "        val_epoch_accuracy = tf.keras.metrics.CategoricalAccuracy('test_accuracy')\n",
        "\n",
        "        # Training\n",
        "        for sub_X, sub_Y in tqdm.tqdm(data_train.generate_data(),total = n_train/batch_size):\n",
        "#            print(f\"sub_X: {sub_X.shape}\")\n",
        "#            print(f\"sub_Y: {sub_Y.shape}\")\n",
        "#            training = ((sub_X.astype(np.float32) - mean)/std)[:,:,[3,0,1,2]]\n",
        "            training = sub_X.astype(np.float32)[:,:,[3,0,1,2]]\n",
        "            target = sub_Y.astype(np.float32)[:,-6:-1]\n",
        "            def grad(model, input_par, targets):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_value = loss(model, input_par, targets)\n",
        "                return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
        "            \n",
        "            # Define optimizer\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "            # Compute loss and gradients\n",
        "            loss_value, grads = grad(gnn, training, target)\n",
        "            \n",
        "            # Update the gradients\n",
        "            optimizer.apply_gradients(zip(grads, gnn.trainable_variables))\n",
        "            \n",
        "            # Track progress\n",
        "            epoch_loss_avg(loss_value)  # Add current batch loss\n",
        "            # Compare predicted label to actual label\n",
        "            epoch_accuracy(target, tf.nn.softmax(gnn(training)))\n",
        "\n",
        "        # Validation\n",
        "        for sub_X, sub_Y in tqdm.tqdm(data_val.generate_data(),total = n_val/batch_size):\n",
        "            #training = ((sub_X.astype(np.float32) - mean)/std)[:,:,[3,0,1,2]]\n",
        "            training = (sub_X.astype(np.float32))[:,:,[3,0,1,2]]\n",
        "            target = sub_Y.astype(np.float32)[:,-6:-1]\n",
        "            \n",
        "            # Compute the loss\n",
        "            loss_value = loss(gnn, training, target)\n",
        "            \n",
        "            # Track progress\n",
        "            val_epoch_loss_avg(loss_value)\n",
        "            val_epoch_accuracy(target, tf.nn.softmax(gnn(training)))\n",
        "\n",
        "        # End epoch\n",
        "        train_loss_results.append(epoch_loss_avg.result())\n",
        "        train_accuracy_results.append(epoch_accuracy.result())\n",
        "        val_loss_results.append(val_epoch_loss_avg.result())\n",
        "        val_accuracy_results.append(val_epoch_accuracy.result())\n",
        "       \n",
        "        # Save best epoch only\n",
        "        if best_loss > val_epoch_loss_avg.result():\n",
        "            best_loss = val_epoch_loss_avg.result()\n",
        "\n",
        "            # Save the model after training\n",
        "            #pathlib.Path(best_path).mkdir(parents=True, exist_ok=True)  \n",
        "            #gnn.save_weights(best_path, save_format='tf')\n",
        "\n",
        "        # Logs for tensorboard\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', epoch_loss_avg.result(), step=epoch)\n",
        "            tf.summary.scalar('accuracy', epoch_accuracy.result(), step=epoch)\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar('loss', val_epoch_loss_avg.result(), step=epoch)\n",
        "            tf.summary.scalar('accuracy', val_epoch_accuracy.result(), step=epoch)\n",
        "\n",
        "        template = 'Epoch {}, Loss: {:.4f}, Accuracy: {:.2f}%, Test Loss: {:.4f}, Test Accuracy: {:.2f}%'\n",
        "        print (template.format(epoch+1,\n",
        "                         epoch_loss_avg.result(), \n",
        "                         epoch_accuracy.result()*100,\n",
        "                         val_epoch_loss_avg.result(), \n",
        "                         val_epoch_accuracy.result()*100))\n",
        "\n",
        "        # Reset metrics every epoch\n",
        "        epoch_loss_avg.reset_states()\n",
        "        val_epoch_loss_avg.reset_states()\n",
        "        epoch_accuracy.reset_states()\n",
        "        val_epoch_accuracy.reset_states()\n",
        "\n",
        "    # Save the model after training\n",
        "#    pathlib.Path(save_path).mkdir(parents=True, exist_ok=True)  \n",
        "#    gnn.save_weights(save_path, save_format='tf')\n",
        "\n",
        "def evaluate(args):\n",
        "    net_args = (N, n_targets, n_features, args.hidden)\n",
        "    net_kwargs = {\"fr_activation\": 0, \"fo_activation\": 0, \"fc_activation\": 0, \"De\": args.De, \"Do\": args.Do}\n",
        "    \n",
        "    gnn = LEIA(*net_args, **net_kwargs)\n",
        "    gnn.build(input_shape=(None, N, n_features))\n",
        "    gnn.load_weights(best_path)\n",
        "    \n",
        "    data_val.set_file_names(files_val)\n",
        "    n_val=data_val.count_data()\n",
        "        \n",
        "    epoch_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
        "    epoch_accuracy = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
        "    \n",
        "    # Load mean and std for normalization\n",
        "    mean, std = normalize(files_train)\n",
        "\n",
        "    # Validation\n",
        "    for sub_X, sub_Y in tqdm.tqdm(data_val.generate_data(),total = n_val/batch_size):\n",
        "        #training = ((sub_X.astype(np.float32) - mean)/std)[:,:,[3,0,1,2]]\n",
        "        training = sub_X.astype(np.float32)[:,:,[3,0,1,2]]\n",
        "        target = sub_Y.astype(np.float32)[:,-6:-1]\n",
        "        \n",
        "        # Compute the loss\n",
        "        loss_value = loss(gnn, training, target)\n",
        "        \n",
        "        # Track progress\n",
        "        epoch_loss(loss_value)\n",
        "        epoch_accuracy(target, tf.nn.softmax(gnn(training)))\n",
        "        \n",
        "    template = 'Loss: {}, Accuracy: {}%'\n",
        "    print (template.format(epoch_loss.result(), \n",
        "                     epoch_accuracy.result()*100))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" This is executed when run from the command line \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    # Required positional arguments\n",
        "    \n",
        "    # Optional arguments\n",
        "    parser.add_argument(\"--hidden\", type=int, action='store', dest='hidden', default = 128, help=\"hidden parameter\")\n",
        "    parser.add_argument(\"--De\", type=int, action='store', dest='De', default = 64, help=\"De parameter\")\n",
        "    parser.add_argument(\"--Do\", type=int, action='store', dest='Do', default = 64, help=\"Do parameter\")\n",
        "    parser.add_argument(\"--evaluate\", action='store_true', dest='evaluate', default = False, help=\"only run evaluation\")\n",
        "    parser.add_argument(\"--trial\", action='store_true', dest='trial', default = False, help=\"train on a smaller sample\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    if not args.evaluate: main(args)\n",
        "    else: evaluate(args)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}